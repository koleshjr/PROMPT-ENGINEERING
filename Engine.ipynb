{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd353Cbp0cQnlBpYlN1ksG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koleshjr/PROMPT-ENGINEERING/blob/main/Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The main Engine"
      ],
      "metadata": {
        "id": "Uzl8zyNbjUDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "class BraindumpEngine:\n",
        "  '''The main class of the braindump engine. It stores the database and application parameters, as well as coordinates the calls to the GPT-3 model, leveraging the preprocessor and postprocessor. \n",
        "  In this manner, it provides the capability both to insert facts into the database and to query the database'''\n",
        "\n",
        "  def __init__(self, api_key = os.getenv(\"OPENAI_API_KEY\"),\n",
        "               database_file_path = \"./data/default_database.csv\",\n",
        "               categories_file_path = \"./data/default_categories.csv\",\n",
        "               gpt3_engine = \"text-davinci-003\", gpt3_temperatue=0.1,\n",
        "               default_categories=['Family','Work','Friends','Shopping','Health',\n",
        "                                   'Finance','Travel','Home','Pets','Hobbies','Other']):\n",
        "    \n",
        "    self._database_file_path = database_file_path\n",
        "    self._categories_file_path = categories_file_path\n",
        "    self._categories = default_categories\n",
        "\n",
        "    # Load the database or create it from scratch if needed\n",
        "    try:\n",
        "      self.database = pd.read_csv(self.database_file_path)\n",
        "      logging.info(f'Loaded database from {self._database_file_path}.')\n",
        "    except FileNotFoundError:\n",
        "      self.database = pd.DataFrame(columns = ['Category','Type','People','Key','Value'])\n",
        "      self._save()\n",
        "      logging.info(f'Created database in {self._database_file_path}.')\n",
        "\n",
        "    # Load the categories file or create it from scratch if needed\n",
        "\n",
        "    try:\n",
        "      df_categories = pd.read_csv(self._categories_file_path)\n",
        "      self._categories = df_categories['Category'].tolist()\n",
        "      logging.info(f'Loaded the Categories {self._categories} from {self._categories_file_path}')\n",
        "    except FileNotFoundError:\n",
        "      self.database = pd.DataFrame(default_categories, columns = ['Category'])\n",
        "      self._save()\n",
        "      logging.info(f'Created categories {self._categories} in {self._categories_file_path}')\n",
        "\n",
        "    openai.api_key = api_key\n",
        "    self.gpt3_parameters  = {'engine': gpt3_engine,\n",
        "                             'temperature': gpt3_temperature,\n",
        "                             'max_tokens': 200,\n",
        "                             'top_p': 1.0,\n",
        "                             'frequency_penalty': 0.0,\n",
        "                             'presence_penalty': 0.0,\n",
        "                             'stop': None}\n",
        "\n",
        "    self._current_extracted_facts = None\n",
        "\n",
        "\n",
        "    # Create preprocessor and postprocessor for GPT-3 inputs and outputs, respectively\n",
        "    self._preprocessor = BraindumpPreprocessor()\n",
        "    self._postprocessor = BraindumpPostprocessor()\n",
        "\n",
        "  def _save(self):\n",
        "    logging.info(f'Database has {len(self.database)} facts.')\n",
        "    logging.info(f'Available categories are {self._categories}')\n",
        "\n",
        "    self.database.to_csv(self._database_file_path, index = False)\n",
        "    pd.DataFrame(self._categories, columns=['Category'].to_csv(self._categories_file_path,index = False))\n",
        "\n",
        "    logging.info(f'Saved database in {self._database_file_path}.')\n",
        "    logging.info(f'Saved allowed categories in {self._categories_file_path}.')\n",
        "\n",
        "\n",
        "  ################################\n",
        "  # Facts insertion workflow methods\n",
        "  ###############################\n",
        "\n",
        "  def extract_facts(self, facts_utterance):\n",
        "    ''' Extracts facts from a natural language utterance. Returns a list of tuples (category, type, people, key, value).\n",
        "    '''\n",
        "    fact_tuples = self._postprocessor.string_to_tuples(self._gpt3_complete(self._preprocessor.extraction_prompt(facts_utterance, self._categories)))\n",
        "    self._current_extracted_facts = fact_tuples\n",
        "    return fact_tuples\n",
        "\n",
        "  def has_extracted_facts(self):\n",
        "    return self._current_extracted_facts is not None\n",
        "\n",
        "  def extracted_facts(self):\n",
        "    '''\n",
        "    Returns the current extracted facts as a list of dictionaries, for ease in readability'''\n",
        "\n",
        "    return [{'Category': fact[0], \n",
        "             'Type': fact[1],\n",
        "             'People': fact[2],\n",
        "             'key': fact[3],\n",
        "             'value': fact[4]} for fact in self._current_extracted_facts]\n",
        "\n",
        "  def commit(self):\n",
        "    '''\n",
        "    commits the current extracted facts to the database. If no facts have been extracted\n",
        "    it does nothing'''\n",
        "\n",
        "    if self._current_extracted_facts is not None:\n",
        "      self._insert_facts()\n",
        "      self._current_extracted_facts = None\n",
        "      self._save()\n",
        "\n",
        "    else:\n",
        "      logging.info('Nothing to commit.')           \n",
        "\n",
        "  def cancel(self):\n",
        "    '''\n",
        "    Cancel the current extracted facts or if no facts have been extracted it basically does nothing'''\n",
        "\n",
        "    if self._current_extracted_facts is not None:\n",
        "      self._current_extracted_facts = None\n",
        "\n",
        "    else:\n",
        "      logging.info('Nothing to revert.')\n",
        "\n",
        "  def _insert_facts(self, facts_utterance = None):\n",
        "    '''\n",
        "    Inserts a fact into the database\n",
        "    '''\n",
        "\n",
        "    # reuse the extracted facts, if any\n",
        "\n",
        "    if self.current_extracted_facts is None:\n",
        "      fact_tuples = self.extract_facts(facts_utterance)\n",
        "\n",
        "    else:\n",
        "      fact_tuples = self._current_extracted_facts\n",
        "\n",
        "    for fact_tuple in fact_tuples:\n",
        "      logging.info(f'Database has {len(self.database)} facts before insertion')\n",
        "      logging.info(f'Inserting fact: {fact_tuple}')\n",
        "\n",
        "      df_to_add = pd.DataFrame([fact_tuple], columns = [\"Category\", \"Type\", \"People\", \"Key\", \"Value\"])\n",
        "      self.database = pd.concat([self.database, df_to_add], ignore_index=True)\n",
        "\n",
        "      logging.info(f'Database has {len(self.database)} facts after insertion')\n",
        "\n",
        "\n",
        "\n",
        "  ###############################\n",
        "  # Search workflow methods\n",
        "  ###############################\n",
        "\n",
        "  def query(self, fact_query, categories = None, entry_types = None, people = None,show_none_if_no_query=False, verbose=False):\n",
        "    ''' Queries the database for a fact'''\n",
        "\n",
        "    if len(fact_query) > 0 or show_none_if_no_query:\n",
        "      raw_original_terms = self._gpt3_complete(self._preprocessor.terms_extraction_prompt(fact_query))\n",
        "      original_terms = self._postprocessor.extract_lines_from_result(raw_original_terms)\n",
        "      if verbose:\n",
        "        print(original_terms)\n",
        "\n",
        "      augmented_terms = []\n",
        "      for original_term in original_terms:\n",
        "        raw_augmented_terms = self._gpt3_complete(self._preprocessor.terms_augmentation_prompt(original_term))\n",
        "        augmented_terms += self._postprocessor.extract_lines_from_result(raw_augmented_terms)\n",
        "\n",
        "      if verbose:\n",
        "        print(augmented_terms)\n",
        "\n",
        "      return self._search_dataframe(self._database_filtered_by(categories, entry_types, people),\n",
        "                                    original_terms, augmented_terms)\n",
        "      \n",
        "    else:\n",
        "      return self._database_filtered_by(categories, entry_types, people)\n",
        "  \n",
        "  def _search_dataframe(self, df, original_terms, augmented_terms):\n",
        "    '''\n",
        "    Searches the specified database for the specified terms. '''\n",
        "\n",
        "    df = df.fillna('') #for readability below\n",
        "    all_terms = original_terms + augmented_terms\n",
        "\n",
        "    df_results = None\n",
        "    for column in df.columns:\n",
        "      df_result = df[df[column].str.contains(\"|\".join(all_terms), case = False).fillna(False)]\n",
        "      if df_results is None:\n",
        "        df_results = df_result\n",
        "      else:\n",
        "        df_results = pd.concat([df_results, df_result])\n",
        "\n",
        "    return df_results\n",
        "\n",
        "\n",
        "  def _database_filtered_by(self, categories = None, entry_types = None, people = None):\n",
        "    df = self.database\n",
        "    def aux_filter(df, column, values):\n",
        "      if values is not None and len(values)> 0:\n",
        "        return df [self.database[column].str.lower().isin([v.lower() for v in values])]\n",
        "      else:\n",
        "        return df\n",
        "\n",
        "    df = aux_filter(df,'Category', categories)\n",
        "    df = aux_filter(df, 'Type', entry_types)\n",
        "    df = aux_filter(df, 'people', people)\n",
        "\n",
        "    return df\n",
        "\n",
        "  def unique_categories_in_database(self):\n",
        "    return self.database['Category'].unique().tolist()\n",
        "\n",
        "  def unique_entry_types_in_database(self):\n",
        "    return self.database['Type'].unique().tolist()    \n",
        "\n",
        "  def unique_people_in_database(self):\n",
        "    return self.database['People'].unique().tolist()    \n",
        "\n",
        "  ##############################\n",
        "  # Categories Management\n",
        "  ##############################\n",
        "\n",
        "  def allowed_categories(self):\n",
        "    return self._categories\n",
        "\n",
        "  def update categories(self, new_categories):\n",
        "    self._categories = new_categories\n",
        "    self._save()\n",
        "\n",
        "\n",
        "\n",
        "  ###########################\n",
        "  # GPT-3 API\n",
        "  ###########################\n",
        "\n",
        "  def _gpt3_complete(self prompt, echo = False):\n",
        "    response = openai.Completion.create(\n",
        "        engine = self.gpt3_parameters['engine'],\n",
        "        prompt = prompt\n",
        "        temperature = self.gpt3_parameters['temperature'],\n",
        "        max_tokens = self.gpt3_parameters['max_tokens'],\n",
        "        top_p = self.gpt3_parameters['top_p'],\n",
        "        frequency_penalty = self.gpt3_parameters['frequency_penalty'],\n",
        "        presence_penalty = self.gpt3_parameters['presence_penalty'],\n",
        "        stop = self.gpt3_parameters['stop'],\n",
        "        echo = echo\n",
        "\n",
        "    )    \n",
        "    completion = response['choices'][0]['text']\n",
        "\n",
        "    return completion\n",
        "\n",
        "  def set_openai_api_key(self, key):\n",
        "    openai.api_key = key\n",
        "\n",
        "  \n",
        "  ###########################\n",
        "  # Data Utilities\n",
        "  ##########################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b6PdvTW5jVxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}